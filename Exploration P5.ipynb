{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook de l'exploration et pre-traitement des questions du corpus v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Pre-requis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des librairies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset\n",
    "\n",
    "dataset = pd.read_csv('dataset/stack_overflow_query_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Title          50000 non-null  object \n",
      " 1   Body           50000 non-null  object \n",
      " 2   Tags           50000 non-null  object \n",
      " 3   Id             50000 non-null  int64  \n",
      " 4   Score          50000 non-null  int64  \n",
      " 5   ViewCount      50000 non-null  int64  \n",
      " 6   FavoriteCount  0 non-null      float64\n",
      " 7   AnswerCount    50000 non-null  int64  \n",
      "dtypes: float64(1), int64(4), object(3)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.562872e+06</td>\n",
       "      <td>25.357040</td>\n",
       "      <td>2.371003e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.58756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.694544e+05</td>\n",
       "      <td>217.521498</td>\n",
       "      <td>1.506203e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.99613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>2.900000e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.314418e+05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.038000e+03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.614784e+06</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.637000e+03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.239268e+06</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.269000e+03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.776234e+06</td>\n",
       "      <td>20372.000000</td>\n",
       "      <td>1.192372e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Id         Score     ViewCount  FavoriteCount  AnswerCount\n",
       "count  5.000000e+04  50000.000000  5.000000e+04            0.0  50000.00000\n",
       "mean   1.562872e+06     25.357040  2.371003e+04            NaN      3.58756\n",
       "std    7.694544e+05    217.521498  1.506203e+05            NaN      3.99613\n",
       "min    4.000000e+00     -6.000000  2.900000e+01            NaN      0.00000\n",
       "25%    9.314418e+05      1.000000  1.038000e+03            NaN      1.00000\n",
       "50%    1.614784e+06      2.000000  2.637000e+03            NaN      3.00000\n",
       "75%    2.239268e+06      7.000000  8.269000e+03            NaN      4.00000\n",
       "max    2.776234e+06  20372.000000  1.192372e+07            NaN    131.00000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Nettoyage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatage de la variable 'Tags'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyer le formatage de la variables 'Tags' en enlevant '<' puis en supprimant le dernier caractere '>' et en decoupant la chaine par '>'\n",
    "\n",
    "def process_tags(tags):\n",
    "    \"\"\"\n",
    "    Transforme une chaîne de caractères contenant des tags délimités par '<' et '>'.\n",
    "    \n",
    "    Cette fonction remplace les caractères '<' dans la chaîne par des chaînes vides, \n",
    "    enlève le dernier caractère de la chaîne résultante, puis divise la chaîne restante \n",
    "    en une liste de sous-chaînes en utilisant '>' comme délimiteur.\n",
    "\n",
    "    Args:\n",
    "        tags (str): Une chaîne de caractères contenant des tags délimités par '<' et '>'.\n",
    "\n",
    "    Returns:\n",
    "        list: Une liste de chaînes, où chaque chaîne est un tag extrait de l'entrée.\n",
    "    \"\"\"\n",
    "    return tags.replace('<', '')[0:-1].split('>')\n",
    "\n",
    "dataset['Tags'] = dataset['Tags'].apply(process_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppression de la variable 'FavoriteCount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de la variable 'FavoriteCount' du dataset.\n",
    "# Cette variable est vide du fait d'un bug avec Stack Exchange Data Explorer\n",
    "\n",
    "dataset.drop(columns='FavoriteCount', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenisation de la variable 'Title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de tokenisation de la variable 'Title'\n",
    "\n",
    "def gen_token(word):\n",
    "    \"\"\"\n",
    "    Cette fonction prend en entrée une chaîne de caractères (word) et retourne\n",
    "    une liste de tokens générés à partir de cette chaîne en utilisant un \n",
    "    tokenizer basé sur des expressions régulières. Elle conserve également\n",
    "    certains caractères spéciaux comme le '#' et l'apostrophe.\n",
    "\n",
    "    Args:\n",
    "    word (str): La chaîne de caractères à tokenizer.\n",
    "\n",
    "    Returns:\n",
    "    list: Une liste contenant les tokens générés à partir de la chaîne de caractères.\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"[\\w#'.]+\")\n",
    "    return tokenizer.tokenize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# On applique la fonction a la variable 'Title'\n",
    "dataset['title_tokenize'] = dataset['Title'].apply(gen_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenisation de la variable 'Body'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de tokenisation de la variable 'Body'\n",
    "\n",
    "dataset['body_tokenize'] = dataset['Body'].apply(gen_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe le corpus des stopwords pour l'anglais depuis NLTK\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Crée un ensemble de mots à partir du corpus de mots anglais disponible dans NLTK\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "# Initialise un objet lemmatizer de la classe WordNetLemmatizer pour le traitement de lemmatisation\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_text(text):\n",
    "    \"\"\"\n",
    "    Cette fonction prend en entrée un tweet (chaîne de caractères) et retourne\n",
    "    une version prétraitée du tweet après avoir effectué la tokenisation, la\n",
    "    suppression des stopwords, la lemmatisation et le filtrage des tokens ayant moins de 3 caractères.\n",
    "\n",
    "    Args:\n",
    "    text (str): Le tweet à prétraiter.\n",
    "\n",
    "    Returns:\n",
    "    str: Le tweet prétraité sous forme de chaîne de caractères, avec les tokens\n",
    "         filtrés, séparés par des espaces.\n",
    "    \"\"\"\n",
    "\n",
    "    # Enlever les balises HTML\n",
    "    soup = BeautifulSoup(text, \"lxml\").get_text()\n",
    "\n",
    "    # Tokenisation modifiée pour inclure des caractères comme \"#\" et \".\"\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"[\\w#'.]+\").tokenize(soup)\n",
    "\n",
    "    # Supprimer les stopwords\n",
    "    words_w_stopwords = [i for i in tokenizer if i not in stopwords]\n",
    "\n",
    "    # Lemmatisation\n",
    "    words_lemmatize = (lemmatizer.lemmatize(w) for w in words_w_stopwords)\n",
    "\n",
    "    # Supprimer les tokens de moins de 3 caractères mais conserver ceux avec des caractères spéciaux\n",
    "    words_w_stopwords_and_3char = [w.lower() for w in words_lemmatize if len(w) >= 2]\n",
    "\n",
    "    return ' '.join(words_w_stopwords_and_3char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing de la variable 'Body'\n",
    "\n",
    "dataset['preproc_body'] = dataset['Body'].apply(preproc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b8/t710d8ys4x55cg_q12zdpnf00000gn/T/ipykernel_21499/2928602134.py:16: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"lxml\").get_text()\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing de la variable 'Title'\n",
    "\n",
    "dataset['preproc_title'] = dataset['Title'].apply(preproc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [dataset['preproc_title']]\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le mot 'gettimeofday' n'est pas présent dans le vocabulaire.\n"
     ]
    }
   ],
   "source": [
    "word = 'gettimeofday'\n",
    "if word in model.wv:\n",
    "    print(f\"Vecteur pour le mot '{word}': {model.wv[word]}\")\n",
    "else:\n",
    "    print(f\"Le mot '{word}' n'est pas présent dans le vocabulaire.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet-Pfk8s4i7-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
